{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See our docs for an explanation of what this code is doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from docent import Docent\n",
    "\n",
    "client = Docent(\n",
    "    api_key=os.getenv(\"DOCENT_API_KEY\"),  # is default and can be omitted\n",
    "\n",
    "    # Uncomment and adjust these if you're self-hosting\n",
    "    # server_url=\"http://localhost:8889\",\n",
    "    # web_url=\"http://localhost:3001\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = client.create_collection(name=\"inspect example\", description=\"example inspect log that comes with the Docent repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docent.samples import get_inspect_fpath\n",
    "from inspect_ai.log import read_eval_log\n",
    "from pydantic_core import to_jsonable_python\n",
    "\n",
    "ctf_log = read_eval_log(get_inspect_fpath())\n",
    "ctf_log_dict = to_jsonable_python(ctf_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inspect_ai.log import EvalLog\n",
    "from docent.data_models import AgentRun, Transcript\n",
    "from docent.data_models.chat import parse_chat_message\n",
    "\n",
    "def load_inspect_log(log: EvalLog) -> list[AgentRun]:\n",
    "    if log.samples is None:\n",
    "        return []\n",
    "\n",
    "    agent_runs: list[AgentRun] = []\n",
    "\n",
    "    for s in log.samples:\n",
    "        # Extract sample_id from the sample ID\n",
    "        sample_id = s.id\n",
    "        epoch_id = s.epoch\n",
    "\n",
    "        # Gather scores\n",
    "        scores: dict[str, int | float | bool | None] = {}\n",
    "\n",
    "        # Evaluate correctness (for this CTF benchmark)\n",
    "        if s.scores and \"includes\" in s.scores:\n",
    "            scores[\"correct\"] = s.scores[\"includes\"].value == \"C\"\n",
    "\n",
    "        # Set metadata\n",
    "        metadata = {\n",
    "            \"task_id\": log.eval.task,\n",
    "            \"sample_id\": str(sample_id),\n",
    "            \"epoch_id\": epoch_id,\n",
    "            \"model\": log.eval.model,\n",
    "            \"scores\": scores,\n",
    "            \"additional_metadata\": s.metadata,\n",
    "            \"scoring_metadata\": s.scores,\n",
    "        }\n",
    "\n",
    "        # Create transcript\n",
    "        agent_runs.append(\n",
    "            AgentRun(\n",
    "                transcripts=[\n",
    "                    Transcript(\n",
    "                        messages=[parse_chat_message(m.model_dump()) for m in s.messages]\n",
    "                    )\n",
    "                ],\n",
    "                metadata=metadata,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return agent_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_runs = load_inspect_log(ctf_log)\n",
    "print(agent_runs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.add_agent_runs(collection_id, agent_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
